---
title: "MovieLens Recomendation system Project"
author: "Matias Garcia Mazzaro"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:  
  pdf_document:
    fig_crop: no
    number_sections: yes
    toc: yes
    toc_depth: 3
geometry:
- top=25mm
- bottom=25mm
- left=25mm
- right=25mm
- heightrounded
highlight-style: pygments
linkcolor: blue
mainfont: Arial
fontsize: 12pt
sansfont: Verdana
documentclass: report
urlcolor: blue
---
\pagebreak
# The Project {-}

This is a Machine Learning Project named MovieLens, it is a recomendation system of movies based in 10 millons dataset. The sourse of the dataset in this work is provided for [Group Lens](https://grouplens.org/datasets/movielens/10m/), for download, press [here!!](http://files.grouplens.org/datasets/movielens/ml-10m.zip)

This work is part of the final project for the Data Science Professional certificate of HarvardX. In October 2006, Netflix offered a challenge to the data science community: improve our recommendation algorithm by 10% and win a million dollars.

# The Objetive {-}

We will explore the data and construct a Machine Learning algorithm, describing the complete process, the method to compare diferents models will be trough RMSE (root mean squared error) between the true data and predicted data. 

We need create a data set that we will work and a validation data set. But we need to be sure don't use the validation dataset for define the model. For this cuestion, we will create a data partition of the training data for test it and define the model.

We need obtain a RMSE < 0.86490 in the final validation for the total calification.


# Download and construct the dataset

The Netflix data is not publicly available, but the GroupLens research lab generated their own database with over 20 million ratings for over 27,000 movies by more than 138,000 users.

We start with the cration of **edx** and **validation** objects and loading the packages and libraries we will use.

```{r, warning=FALSE, message=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

# Analysis

It's important don't use the the **validation** data for train and test, it is because we need a eficient model for the data in the future, the final objetive is recomend movies to users, the true rating will ocurr after our recomendation.
The fisrt step, is to take **edx** object and make a data partition **train_set** and **test_set**:

## Data partition

```{r}
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, 
                                  list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```

for be sure there are the same users and movies in the test set and training set;

```{r}
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```

### Exploring the data before to start 

For explore the dataset, we start for know how many diferents movies are in the dataset, and how many diferents users.

```{r}
train_set %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))
```

Also we can observe that the "genres" variable have differents asociated genres.

```{r}
head(train_set$genres)
```

We can observe also the distribution density of ratings.

```{r}
edx %>% group_by(rating) %>% ggplot(aes(rating)) + geom_density()
```

## Creation of the RMSE function

With the porpouse of messure the _root mean squared error_ (RMSE), we define;  

$$RMSE=\sqrt{\frac{1}{N}\sum_{u,i}(\hat{y}_{u,i}-y_{u,i})^2}$$

Where **y** is the rating for each movie **_i_** by user **_u_** and our prediction, and **N** being the number of user/movie combinations and the sum occurring over all these combinations.

```{r, echo= TRUE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}  
```

The RMSE is similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If this number is larger than 1, it means our typical error is larger than one star, which is not good.

## The simplest model

The simplest possible recommendation system is the same rating for all movies regardless of user. A model that assumes the same rating for all movies and users with all the differences explained by random variation would look like this:

$$y_{u,i} = \mu + \epsilon_{u,i}$$

With $\epsilon$ independent errors sampled from the same distribution centered at 0 and $\mu$ the “true” rating for all movies.

```{r}
mu_hat <- mean(train_set$rating)
mu_hat

naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse
```

We save the result in a new object for compare results later.

```{r}
rmse_results <- tibble(method = "Just the average", RMSE = naive_rmse)
```

## _Modeling movie effect_

For model the _movie effect_, which implies, a *bias* added to the before model who move the mean for the mean of movie, we write;

$$y_{u,i} = \mu + bi + \epsilon_{u,i}$$

```{r, message=FALSE}
mu <- mean(train_set$rating) 

movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
```

The distribution of bias is;

```{r, echo=FALSE}
qplot(b_i, data = movie_avgs, bins = 10, color = I("black"))
```

We predict and test;

```{r}
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

movie_effect <- RMSE(predicted_ratings, test_set$rating)
movie_effect
```

We can see, the model including movie effect is better.

## _Modeling user effect_

We add to the before model the user effect:

$$y_{u,i} = \mu + bi + bu + \epsilon_{u,i}$$

We observe the distribution for diferents users;

```{r, echo=FALSE}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")
```

We calculate de average for users;

```{r, message=FALSE}
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

And predict movie effect adding user effect.

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

user_effect <- RMSE(predicted_ratings, test_set$rating)
user_effect
```

Sustancially diferent with others models.

## Regularization models

If we explore the data for observe mistakes, we can see easily, there are movies with a fews ratings. Regularization permits us to penalize large estimates that are formed using small sample sizes.

For that we test which Lambda is the optimal ones that aport the minimal RMSE;

```{r, message=FALSE, warning=FALSE}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})
```

We plot the Lambdas & the rmses for see how this affect, then we print the Lambda for the minimum RMSE.

```{r}
qplot(lambdas, rmses)  

lambda_opt <- lambdas[which.min(rmses)]
lambda_opt

regularizated_movie_user <- min(rmses)
```

The optimal Lambda is 4.75 . 

```{r}
min(rmses)
```

And the rmse for Lambda 4.75 is 0.86524.

So, now we have this differents RMSEs

```{r, echo=FALSE}
rmse_results <- tibble(method = c("Just the average", "Movie effect Model", "Movie + User effect Model", "Regularized Movie + User effect Model"), RMSE = c(naive_rmse, movie_effect, user_effect, regularizated_movie_user)) %>% mutate(RMSE = sprintf("%0.5f", RMSE))
rmse_results
```

# Improve it.

If we want to do this better, we need to think about the data. We observe, the date where the movie was filmed and the date where was ranked for each users.
Another information we can consider is the gendres of the movies. For all this we need to process original data.

## Data transformation


### Dates transformation

If we look inside the data, we can observe all the titles of the films have the year were was released.

```{r}
head(train_set$title)
```

The first step is to add a column with the extraction of this year and count the oldness of the film, and then check it;

```{r}
year_movie <-as.numeric(str_sub(train_set$title, start = -5, end = -2))
train_set <- train_set %>% mutate(year = year_movie)

year_movie <-as.numeric(str_sub(test_set$title, start = -5, end = -2))
test_set <- test_set %>% mutate(year = year_movie)

year_movie <-as.numeric(str_sub(validation$title, start = -5, end = -2))
validation <- validation %>% mutate(year = year_movie)

year_movie <-as.numeric(str_sub(edx$title, start = -5, end = -2))
edx <- edx %>% mutate(year = year_movie)

validation <- validation %>% mutate(Age = 2020 - year)
train_set <- train_set %>% mutate(Age = 2020 - year)
test_set <- test_set %>% mutate(Age = 2020 - year)
edx <- edx %>% mutate(Age = 2020 - year)

head(train_set)
```

Also we need to convert the timestamp in a year of rating.

```{r}
class(validation$timestamp)

validation <- validation %>% mutate(timestamp = as.Date.POSIXct(timestamp))
validation <- validation %>% mutate(year_rating = year(timestamp))

train_set <- train_set %>% mutate(timestamp = as.Date.POSIXct(timestamp))
train_set <- train_set %>% mutate(year_rating = year(timestamp))

test_set <- test_set %>% mutate(timestamp = as.Date.POSIXct(timestamp))
test_set <- test_set %>% mutate(year_rating = year(timestamp))

edx <- edx %>% mutate(timestamp = as.Date.POSIXct(timestamp))
edx <- edx %>% mutate(year_rating = year(timestamp))

head(validation)
```

### Genres transformation

We start we separate the genres and looks;

```{r}
genres <- train_set %>% separate_rows(genres, sep = "\\|")

table_genres <- genres %>% group_by(genres) %>% summarize(n = n(), mean = mean(rating)) %>% arrange(desc(n))

table_genres %>% mutate(genres = reorder(genres, mean)) %>% ggplot(aes(genres,mean,size = n, color = n)) + geom_count() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Genres Vs Means and rating") + xlab("Genres") + ylab("Means")
```

We can observe how diferents genres affect the mean ratings.

```{r}
table_genres %>% mutate(dif = mean - mean(mean))
```

## Adding the Oldness Movie effect

```{r}

mu <- mean(train_set$rating)
  
b_i <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+4.75))
  
b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+4.75))

b_age <- train_set %>%
  left_join(b_i, by="movieId") %>% 
  left_join(b_u, by ="userId") %>%
  group_by(Age) %>% 
  summarize(b_age = mean(rating - b_i - b_u - mu)/(n()+4.75))
  
predicted_ratings <- 
  test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_age, by = "Age") %>%
  mutate(pred = mu + b_i + b_u + b_age) %>%
  pull(pred)

regu_user_movie_age <- RMSE(predicted_ratings, test_set$rating)
```

and compare the results with the before models;

```{r}
rmse_results <- tibble(method = c("Just the average", "Movie effect Model", "Movie + User effect Model", "Regularized Movie + User effect Model", "Regularized Movie + User + Age effect Model"), RMSE = c(naive_rmse, movie_effect, user_effect, regularizated_movie_user, regu_user_movie_age)) %>% mutate(RMSE = sprintf("%0.5f", RMSE))
rmse_results
```

There is not significant results by oldness movie.

## Moving the average in a function

Like we optimize the parameter "Lambda", we will move the mean near the average $\mu$, is posible there a value who report a better result for the RMSE. We plot the $\mu$ vs. rmses.

```{r, message=FALSE, warning= FALSE}

mu <- mean(train_set$rating)+seq(-0.2, 0.7, 0.05)

rmses <- sapply(mu, function(mu){
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+4.75))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+4.75))
  b_age <- train_set %>%
    left_join(b_i, by="movieId") %>% 
    left_join(b_u, by ="userId") %>%
    group_by(Age) %>% 
    summarize(b_age = mean(rating - b_i - b_u - mu)/(n()+4.75))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_age, by = "Age") %>%
    mutate(pred = mu + b_i + b_u + b_age) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})

qplot(mu, rmses)  
```

We can see there are a optimal $\mu$ over the average.

```{r}
mu <- mu[which.min(rmses)]
mu
regu_user_movie_age_mu <- min(rmses)
```

We add this change in our table.

```{r}
rmse_results <- tibble(method = c("Just the average", "Movie effect Model", "Movie + User effect Model", "Regularized Movie + User effect Model", "Regularized Movie + User + Age effect Model", "Reg. Movie + User + Age effect Model (best mu)"), RMSE = c(naive_rmse, movie_effect, user_effect, regularizated_movie_user, regu_user_movie_age, regu_user_movie_age_mu)) %>% mutate(RMSE = sprintf("%0.5f", RMSE))
rmse_results
```

The new RMSE is better, but in not suficient.

## Add the genre to the model

We will try modeling also the genre, but, instead for each individual genres, for the combination of them. We can see there are near 800 diferent combinations.

```{r}
train_set %>% group_by(genres) %>% summarize(b_gen = mu-mean(rating))
```

We training;

```{r}
mu <- mean(train_set$rating)

b_i <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+4.75))

b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+4.75))

b_age <- train_set %>%
  left_join(b_i, by="movieId") %>% 
  left_join(b_u, by ="userId") %>%
  group_by(Age) %>% 
  summarize(b_age = sum(rating - b_i - b_u - mu)/(n()+4.75))

b_gen <- train_set %>% 
  left_join(b_i, by="movieId") %>% 
  left_join(b_u, by ="userId") %>%
  left_join(b_age, by = "Age") %>%
  group_by(genres) %>% 
  summarize(b_gen = sum(rating - b_i - b_u - b_age - mu)/(n()+4.75))

predicted_ratings <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_age, by = "Age") %>%
  left_join(b_gen, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_age) %>%
  pull(pred)

regu_user_movie_age_gen <- RMSE(predicted_ratings, test_set$rating)
```

We explore the results.

```{r}
rmse_results <- tibble(method = c("Just the average", "Movie effect Model", "Movie + User effect Model", "Regularized Movie + User effect Model", "Regularized Movie + User + Age effect Model", "Reg. Movie + User + Age + Genre effect Model"), RMSE = c(naive_rmse, movie_effect, user_effect, regularizated_movie_user, regu_user_movie_age, regu_user_movie_age_gen)) %>% mutate(RMSE = sprintf("%0.5f", RMSE))
rmse_results
```

Now we obtain a significant result.

We fit again the $\mu$ parameter for more presicion.

```{r, message=FALSE, warning=FALSE}
mu <- mean(train_set$rating)+seq(0.1, 0.3, 0.05)


rmses <- sapply(mu, function(mu){
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+4.75))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+4.75))
  
  b_age <- train_set %>%
    left_join(b_i, by="movieId") %>% 
    left_join(b_u, by ="userId") %>%
    group_by(Age) %>% 
    summarize(b_age = sum(rating - b_i - b_u - mu)/(n()+4.75))
  
  b_gen <- train_set %>% 
    left_join(b_i, by="movieId") %>% 
    left_join(b_u, by ="userId") %>%
    left_join(b_age, by = "Age") %>%
    group_by(genres) %>% 
    summarize(b_gen = sum(rating - b_i - b_u - b_age - mu)/(n()+4.75))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_age, by = "Age") %>%
    left_join(b_gen, by = "genres") %>%
    mutate(pred = mu + b_i + b_u + b_age) %>%
    pull(pred)

  return(RMSE(predicted_ratings, test_set$rating))
})

qplot(mu, rmses)  
```

```{r}
mu[which.min(rmses)]

regu_user_movie_age_gen_mu <- min(rmses)

rmse_results <- tibble(method = c("Just the average", "Movie effect Model", "Movie + User effect Model", "Regularized Movie + User effect Model", "Regularized Movie + User + Age effect Model", "Reg. Movie + User + Age + Genre effect Model", "Reg. Movie + User + Age + Genre effect Model (mu opt)"), RMSE = c(naive_rmse, movie_effect, user_effect, regularizated_movie_user, regu_user_movie_age, regu_user_movie_age_gen, regu_user_movie_age_gen_mu)) %>% mutate(RMSE = sprintf("%0.5f", RMSE))
rmse_results
```

# Validation

For test our best model, the last one, we need to process now with the validation set. For train our model we will use the complete training set "edx". Like edx have more data, can be posible a improvement respect our last training.

We use our optimized parameters.

```{r, message=FALSE, warning=FALSE}
mu <- 3.712482

b_i <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+4.75))
  
b_u <- edx %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+4.75))
  
b_age <- edx %>%
  left_join(b_i, by="movieId") %>% 
  left_join(b_u, by ="userId") %>%
  group_by(Age) %>% 
  summarize(b_age = sum(rating - b_i - b_u - mu)/(n()+4.75))
  
b_gen <- edx %>% 
  left_join(b_i, by="movieId") %>% 
  left_join(b_u, by ="userId") %>%
  left_join(b_age, by = "Age") %>%
  group_by(genres) %>% 
  summarize(b_gen = sum(rating - b_i - b_u - b_age - mu)/(n()+4.75))
  
predicted_ratings <- validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_age, by = "Age") %>%
  left_join(b_gen, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + b_age) %>%
  pull(pred)

validation_rmse <- RMSE(predicted_ratings, validation$rating)
validation_rmse
```

We obtain a RMSE of our prediction and true data, of 0.8645024.

```{r}
rmse_results <- tibble(method = c("Just the average", "Movie effect Model", "Movie + User effect Model", "Regularized Movie + User effect Model", "Regularized Movie + User + Age effect Model", "Reg. Movie + User + Age + Genre effect Model", "Reg. Movie + User + Age + Genre effect Model (mu opt)", "final model validation"), RMSE = c(naive_rmse, movie_effect, user_effect, regularizated_movie_user, regu_user_movie_age, regu_user_movie_age_gen, regu_user_movie_age_gen_mu, validation_rmse)) %>% mutate(RMSE = sprintf("%0.5f", RMSE))
rmse_results
```

# Results {-}

In the final validation, were obtain a RMSE of **0.86450** , it is better of the objetive (0.86490).

# Conlusions {-}

In the Netflix challenge the winning score was RMSE=0.8712.
Data exploration is a process, we start with a very simple model and advance in more complex models trought we were knowing the data.

I obtain the objetive RMSE with a computational power of a desktop computer, and always is posible move the average in the base of the model for optimize, denote a coarse fit. Its posible improve it, ensemble models, and moving parameters, also using models like Random Forest, Matriz Factorization, SVD and PCA. 

 


